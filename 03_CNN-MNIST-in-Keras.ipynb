{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Tensorflow (GPU)","language":"python","name":"py3.6-tfgpu"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"},"colab":{"name":"CNN-MNIST-in-Keras.ipynb","provenance":[{"file_id":"https://github.com/AviatorMoser/keras-mnist-tutorial/blob/master/MNIST%20in%20Keras.ipynb","timestamp":1618222968835}]}},"cells":[{"cell_type":"code","metadata":{"collapsed":true,"id":"oB_aEmV9mhz-"},"source":["%matplotlib inline"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aa4sn9fYmh0T"},"source":["# Introducing Convolution! What is it?"]},{"cell_type":"markdown","metadata":{"id":"UrkvQCxymh0T"},"source":["Before, we built a network that accepts the normalized pixel values of each value and operates soley on those values. What if we could instead feed different features (e.g. **curvature, edges**) of each image into a network, and have the network learn which features are important for classifying an image?\n","\n","This possible through convolution! Convolution applies **kernels** (filters) that traverse through each image and generate **feature maps**."]},{"cell_type":"markdown","metadata":{"id":"ZbxLM1Gimh0U"},"source":["In the above example, the image is a 5 x 5 matrix and the kernel going over it is a 3 x 3 matrix. A dot product operation takes place between the image and the kernel and the convolved feature is generated. Each kernel in a CNN learns a different characteristic of an image.\n","\n","Kernels are often used in photoediting software to apply blurring, edge detection, sharpening, etc."]},{"cell_type":"markdown","metadata":{"id":"nDVO0GFXmh0U"},"source":["<img src = 'kernels.png' >"]},{"cell_type":"markdown","metadata":{"id":"TaEj7ioFmh0U"},"source":["Kernels in deep learning networks are used in similar ways, i.e. highlighting some feature. Combined with a system called **max pooling**, the non-highlighted elements are discarded from each feature map, leaving only the features of interest, reducing the number of learned parameters, and decreasing the computational cost (e.g. system memory)."]},{"cell_type":"markdown","metadata":{"id":"ZJa_8hNvmh0U"},"source":["<img src = 'max_pooling.png' >"]},{"cell_type":"markdown","metadata":{"id":"1WIIeEy-mh0V"},"source":["We can also take convolutions of convolutions -- we can stack as many convolutions as we want, as long as there are enough pixels to fit a kernel.\n","\n","*Warning: What you may find down there in those deep convolutions may not appear recognizable to you.*"]},{"cell_type":"markdown","metadata":{"id":"SUl5YMY6mh0V"},"source":["<img src = 'go_deeper.jpg' >"]},{"cell_type":"markdown","metadata":{"id":"gNJHeiWTmh0V"},"source":["## Building a \"Deep\" Convolutional Neural Network"]},{"cell_type":"code","metadata":{"collapsed":true,"id":"F930cJOnmh0V","executionInfo":{"status":"ok","timestamp":1618223857880,"user_tz":-120,"elapsed":695,"user":{"displayName":"Marcel","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgnqgvB1-dXF9xtSsw7hpY5o7BOISI-Okj9toVN=s64","userId":"00210246741827932529"}}},"source":["# import some additional tools\n","\n","from keras.preprocessing.image import ImageDataGenerator\n","from keras.layers import Conv2D, MaxPooling2D, ZeroPadding2D, GlobalAveragePooling2D, Flatten\n","from keras.layers.normalization import BatchNormalization\n","from keras.datasets import mnist     # MNIST dataset is included in Keras"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"id":"5nmV5ishmh0V","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618223860659,"user_tz":-120,"elapsed":1087,"user":{"displayName":"Marcel","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgnqgvB1-dXF9xtSsw7hpY5o7BOISI-Okj9toVN=s64","userId":"00210246741827932529"}},"outputId":"54e85933-93e2-4b02-d3cd-60f77d06494c"},"source":["# Reload the MNIST data\n","(X_train, y_train), (X_test, y_test) = mnist.load_data()"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n","11493376/11490434 [==============================] - 0s 0us/step\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"WodMbjiMmh0W","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618223866314,"user_tz":-120,"elapsed":862,"user":{"displayName":"Marcel","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgnqgvB1-dXF9xtSsw7hpY5o7BOISI-Okj9toVN=s64","userId":"00210246741827932529"}},"outputId":"533dfb3a-914b-40b2-99ba-a75afc2f06ac"},"source":["# Again, do some formatting\n","# Except we do not flatten each image into a 784-length vector because we want to perform convolutions first\n","\n","X_train = X_train.reshape(60000, 28, 28, 1) #add an additional dimension to represent the single-channel\n","X_test = X_test.reshape(10000, 28, 28, 1)\n","\n","X_train = X_train.astype('float32')         # change integers to 32-bit floating point numbers\n","X_test = X_test.astype('float32')\n","\n","X_train /= 255                              # normalize each value for each pixel for the entire vector for each input\n","X_test /= 255\n","\n","print(\"Training matrix shape\", X_train.shape)\n","print(\"Testing matrix shape\", X_test.shape)"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Training matrix shape (60000, 28, 28, 1)\n","Testing matrix shape (10000, 28, 28, 1)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"collapsed":true,"id":"3hXOsXgImh0W","executionInfo":{"status":"ok","timestamp":1618223892354,"user_tz":-120,"elapsed":694,"user":{"displayName":"Marcel","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgnqgvB1-dXF9xtSsw7hpY5o7BOISI-Okj9toVN=s64","userId":"00210246741827932529"}}},"source":["from keras.utils import np_utils                         # NumPy related tools\n","# one-hot format classes\n","\n","nb_classes = 10 # number of unique digits\n","\n","Y_train = np_utils.to_categorical(y_train, nb_classes)\n","Y_test = np_utils.to_categorical(y_test, nb_classes)"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"id":"eRQHHLedmh0W","executionInfo":{"status":"ok","timestamp":1618223918675,"user_tz":-120,"elapsed":1042,"user":{"displayName":"Marcel","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgnqgvB1-dXF9xtSsw7hpY5o7BOISI-Okj9toVN=s64","userId":"00210246741827932529"}}},"source":["from keras.models import Sequential  # Model type to be used\n","from keras.layers.core import Dense, Dropout, Activation # Types of layers to be used in our model\n","\n","model = Sequential()                                 # Linear stacking of layers\n","\n","# Convolution Layer 1\n","model.add(Conv2D(32, (3, 3), input_shape=(28,28,1))) # 32 different 3x3 kernels -- so 32 feature maps\n","model.add(BatchNormalization(axis=-1))               # normalize each feature map before activation\n","convLayer01 = Activation('relu')                     # activation\n","model.add(convLayer01)\n","\n","# Convolution Layer 2\n","model.add(Conv2D(32, (3, 3)))                        # 32 different 3x3 kernels -- so 32 feature maps\n","model.add(BatchNormalization(axis=-1))               # normalize each feature map before activation\n","model.add(Activation('relu'))                        # activation\n","convLayer02 = MaxPooling2D(pool_size=(2,2))          # Pool the max values over a 2x2 kernel\n","model.add(convLayer02)\n","\n","# Convolution Layer 3\n","model.add(Conv2D(64,(3, 3)))                         # 64 different 3x3 kernels -- so 64 feature maps\n","model.add(BatchNormalization(axis=-1))               # normalize each feature map before activation\n","convLayer03 = Activation('relu')                     # activation\n","model.add(convLayer03)\n","\n","# Convolution Layer 4\n","model.add(Conv2D(64, (3, 3)))                        # 64 different 3x3 kernels -- so 64 feature maps\n","model.add(BatchNormalization(axis=-1))               # normalize each feature map before activation\n","model.add(Activation('relu'))                        # activation\n","convLayer04 = MaxPooling2D(pool_size=(2,2))          # Pool the max values over a 2x2 kernel\n","model.add(convLayer04)\n","model.add(Flatten())                                 # Flatten final 4x4x64 output matrix into a 1024-length vector\n","\n","# Fully Connected Layer 5\n","model.add(Dense(512))                                # 512 FCN nodes\n","model.add(BatchNormalization())                      # normalization\n","model.add(Activation('relu'))                        # activation\n","\n","# Fully Connected Layer 6                       \n","model.add(Dropout(0.2))                              # 20% dropout of randomly selected nodes\n","model.add(Dense(10))                                 # final 10 FCN nodes\n","model.add(Activation('softmax'))                     # softmax activation"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"LMKrOL_Nmh0W","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618223931039,"user_tz":-120,"elapsed":520,"user":{"displayName":"Marcel","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgnqgvB1-dXF9xtSsw7hpY5o7BOISI-Okj9toVN=s64","userId":"00210246741827932529"}},"outputId":"830b4e7b-c265-4801-cfaa-15b8866592ee"},"source":["model.summary()"],"execution_count":10,"outputs":[{"output_type":"stream","text":["Model: \"sequential\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","conv2d (Conv2D)              (None, 26, 26, 32)        320       \n","_________________________________________________________________\n","batch_normalization (BatchNo (None, 26, 26, 32)        128       \n","_________________________________________________________________\n","activation (Activation)      (None, 26, 26, 32)        0         \n","_________________________________________________________________\n","conv2d_1 (Conv2D)            (None, 24, 24, 32)        9248      \n","_________________________________________________________________\n","batch_normalization_1 (Batch (None, 24, 24, 32)        128       \n","_________________________________________________________________\n","activation_1 (Activation)    (None, 24, 24, 32)        0         \n","_________________________________________________________________\n","max_pooling2d (MaxPooling2D) (None, 12, 12, 32)        0         \n","_________________________________________________________________\n","conv2d_2 (Conv2D)            (None, 10, 10, 64)        18496     \n","_________________________________________________________________\n","batch_normalization_2 (Batch (None, 10, 10, 64)        256       \n","_________________________________________________________________\n","activation_2 (Activation)    (None, 10, 10, 64)        0         \n","_________________________________________________________________\n","conv2d_3 (Conv2D)            (None, 8, 8, 64)          36928     \n","_________________________________________________________________\n","batch_normalization_3 (Batch (None, 8, 8, 64)          256       \n","_________________________________________________________________\n","activation_3 (Activation)    (None, 8, 8, 64)          0         \n","_________________________________________________________________\n","max_pooling2d_1 (MaxPooling2 (None, 4, 4, 64)          0         \n","_________________________________________________________________\n","flatten (Flatten)            (None, 1024)              0         \n","_________________________________________________________________\n","dense (Dense)                (None, 512)               524800    \n","_________________________________________________________________\n","batch_normalization_4 (Batch (None, 512)               2048      \n","_________________________________________________________________\n","activation_4 (Activation)    (None, 512)               0         \n","_________________________________________________________________\n","dropout (Dropout)            (None, 512)               0         \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 10)                5130      \n","_________________________________________________________________\n","activation_5 (Activation)    (None, 10)                0         \n","=================================================================\n","Total params: 597,738\n","Trainable params: 596,330\n","Non-trainable params: 1,408\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"collapsed":true,"id":"nbSXYPuEmh0W","executionInfo":{"status":"ok","timestamp":1618223935335,"user_tz":-120,"elapsed":506,"user":{"displayName":"Marcel","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgnqgvB1-dXF9xtSsw7hpY5o7BOISI-Okj9toVN=s64","userId":"00210246741827932529"}}},"source":["# we'll use the same optimizer\n","\n","model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"id":"x5G67JyUmh0W","executionInfo":{"status":"ok","timestamp":1618223936979,"user_tz":-120,"elapsed":496,"user":{"displayName":"Marcel","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgnqgvB1-dXF9xtSsw7hpY5o7BOISI-Okj9toVN=s64","userId":"00210246741827932529"}}},"source":["# data augmentation prevents overfitting by slightly changing the data randomly\n","# Keras has a great built-in feature to do automatic augmentation\n","\n","gen = ImageDataGenerator(rotation_range=8, width_shift_range=0.08, shear_range=0.3,\n","                         height_shift_range=0.08, zoom_range=0.08)\n","\n","test_gen = ImageDataGenerator()"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"id":"UieUjMyjmh0X","executionInfo":{"status":"ok","timestamp":1618223938880,"user_tz":-120,"elapsed":514,"user":{"displayName":"Marcel","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgnqgvB1-dXF9xtSsw7hpY5o7BOISI-Okj9toVN=s64","userId":"00210246741827932529"}}},"source":["# We can then feed our augmented data in batches\n","# Besides loss function considerations as before, this method actually results in significant memory savings\n","# because we are actually LOADING the data into the network in batches before processing each batch\n","\n","# Before the data was all loaded into memory, but then processed in batches.\n","\n","train_generator = gen.flow(X_train, Y_train, batch_size=128)\n","test_generator = test_gen.flow(X_test, Y_test, batch_size=128)"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"C6HPNN2Ymh0X","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618224828094,"user_tz":-120,"elapsed":885217,"user":{"displayName":"Marcel","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgnqgvB1-dXF9xtSsw7hpY5o7BOISI-Okj9toVN=s64","userId":"00210246741827932529"}},"outputId":"d53b78b7-e87f-4143-bce0-a5538080e865"},"source":["# We can now train our model which is fed data by our batch loader\n","# Steps per epoch should always be total size of the set divided by the batch size\n","\n","# SIGNIFICANT MEMORY SAVINGS (important for larger, deeper networks)\n","\n","model.fit_generator(train_generator, steps_per_epoch=60000//128, epochs=5, verbose=1, \n","                    validation_data=test_generator, validation_steps=10000//128)"],"execution_count":14,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n","  warnings.warn('`Model.fit_generator` is deprecated and '\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch 1/5\n","468/468 [==============================] - 177s 376ms/step - loss: 0.2948 - accuracy: 0.9083 - val_loss: 0.1910 - val_accuracy: 0.9373\n","Epoch 2/5\n","468/468 [==============================] - 177s 378ms/step - loss: 0.0545 - accuracy: 0.9837 - val_loss: 0.0319 - val_accuracy: 0.9905\n","Epoch 3/5\n","468/468 [==============================] - 177s 378ms/step - loss: 0.0410 - accuracy: 0.9882 - val_loss: 0.0296 - val_accuracy: 0.9900\n","Epoch 4/5\n","468/468 [==============================] - 177s 378ms/step - loss: 0.0343 - accuracy: 0.9883 - val_loss: 0.0289 - val_accuracy: 0.9901\n","Epoch 5/5\n","468/468 [==============================] - 177s 377ms/step - loss: 0.0294 - accuracy: 0.9912 - val_loss: 0.0315 - val_accuracy: 0.9904\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.keras.callbacks.History at 0x7fcc5cc15750>"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"code","metadata":{"id":"xi0SReBWmh0X","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618225559910,"user_tz":-120,"elapsed":8517,"user":{"displayName":"Marcel","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgnqgvB1-dXF9xtSsw7hpY5o7BOISI-Okj9toVN=s64","userId":"00210246741827932529"}},"outputId":"91766996-fd04-45fc-d8b2-b75cd1241ae5"},"source":["score = model.evaluate(X_test, Y_test)\n","print('Test score:', score[0])\n","print('Test accuracy:', score[1])"],"execution_count":15,"outputs":[{"output_type":"stream","text":["313/313 [==============================] - 8s 25ms/step - loss: 0.0315 - accuracy: 0.9904\n","Test score: 0.03153061494231224\n","Test accuracy: 0.9904000163078308\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"pKIbJR9qmh0Z"},"source":["#### For a 3D visualization of a very similar network, visit http://scs.ryerson.ca/~aharley/vis/conv/"]}]}