{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CartPole Skating\n",
    "\n",
    "> **Problem**: If Peter wants to escape from the wolf, he needs to be able to move faster than him. We will see how Peter can learn to skate, in particular, to keep balance, using Q-Learning.\n",
    "\n",
    "First, let's install the gym and import required libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyring is skipped due to an exception: org.freedesktop.DBus.Error.InvalidFileContent: D-Bus library appears to be incorrectly set up: see the manual page for dbus-uuidgen to correct this issue. (Failed to open \"/var/lib/dbus/machine-id\": No such file or directory; UUID file '/etc/machine-id' should contain a hex string of length 32, not length 0, with no other text)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: gym in /home/marcel/.local/lib/python3.6/site-packages (0.26.2)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in /home/marcel/.local/lib/python3.6/site-packages (from gym) (0.0.8)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /home/marcel/.local/lib/python3.6/site-packages (from gym) (1.6.0)\n",
      "Requirement already satisfied: numpy>=1.18.0 in /home/marcel/.local/lib/python3.6/site-packages (from gym) (1.19.5)\n",
      "Requirement already satisfied: dataclasses==0.8 in /home/marcel/.local/lib/python3.6/site-packages (from gym) (0.8)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.0 in /home/marcel/.local/lib/python3.6/site-packages (from gym) (4.8.3)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /home/marcel/.local/lib/python3.6/site-packages (from importlib-metadata>=4.8.0->gym) (4.1.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/marcel/.local/lib/python3.6/site-packages (from importlib-metadata>=4.8.0->gym) (3.4.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marcel/.local/lib/python3.6/site-packages/gym/core.py:27: UserWarning: \u001b[33mWARN: Gym minimally supports python 3.6 as the python foundation not longer supports the version, please update your version to 3.7+\u001b[0m\n",
      "  \"Gym minimally supports python 3.6 as the python foundation not longer supports the version, please update your version to 3.7+\"\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!pip install gym \n",
    "\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a cartpole environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(2)\n",
      "Box([-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38], (4,), float32)\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "print(env.action_space)\n",
    "print(env.observation_space)\n",
    "print(env.action_space.sample())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see how the environment works, let's run a short simulation for 100 steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marcel/.local/lib/python3.6/site-packages/gym/envs/classic_control/cartpole.py:212: UserWarning: \u001b[33mWARN: You are calling render method without specifying any render mode. You can specify the render_mode at initialization, e.g. gym(\"CartPole-v1\", render_mode=\"rgb_array\")\u001b[0m\n",
      "  \"You are calling render method without specifying any render mode. \"\n",
      "/home/marcel/.local/lib/python3.6/site-packages/gym/envs/classic_control/cartpole.py:178: UserWarning: \u001b[33mWARN: You are calling 'step()' even though this environment has already returned terminated = True. You should always call 'reset()' once you receive 'terminated = True' -- any further steps are undefined behavior.\u001b[0m\n",
      "  \"You are calling 'step()' even though this \"\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "\n",
    "for i in range(100):\n",
    "   env.render()\n",
    "   env.step(env.action_space.sample())\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During simulation, we need to get observations in order to decide how to act. In fact, `step` function returns us back current observations, reward function, and the `done` flag that indicates whether it makes sense to continue the simulation or not:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.01878172  0.15108874 -0.03655479 -0.29923922] -> 1.0\n",
      "[ 0.02180349 -0.0434936  -0.04253957 -0.0183054 ] -> 1.0\n",
      "[ 0.02093362 -0.23798048 -0.04290568  0.26065814] -> 1.0\n",
      "[ 0.01617401 -0.4324645  -0.03769252  0.5395053 ] -> 1.0\n",
      "[ 0.00752472 -0.23683354 -0.02690241  0.2351883 ] -> 1.0\n",
      "[ 0.00278805 -0.431561   -0.02219864  0.5192655 ] -> 1.0\n",
      "[-0.00584317 -0.23613368 -0.01181334  0.21967083] -> 1.0\n",
      "[-0.01056584 -0.04084487 -0.00741992 -0.07671498] -> 1.0\n",
      "[-0.01138274 -0.23585968 -0.00895422  0.21361773] -> 1.0\n",
      "[-0.01609994 -0.04061086 -0.00468186 -0.08187626] -> 1.0\n",
      "[-0.01691215  0.1545779  -0.00631939 -0.37603265] -> 1.0\n",
      "[-0.01382059 -0.04045373 -0.01384004 -0.08534893] -> 1.0\n",
      "[-0.01462967  0.15486385 -0.01554702 -0.3823661 ] -> 1.0\n",
      "[-0.01153239 -0.04003394 -0.02319434 -0.09462536] -> 1.0\n",
      "[-0.01233307 -0.23481591 -0.02508685  0.19065048] -> 1.0\n",
      "[-0.01702939 -0.42957017 -0.02127384  0.47531515] -> 1.0\n",
      "[-0.02562079 -0.62438536 -0.01176754  0.7612178 ] -> 1.0\n",
      "[-0.0381085  -0.42910326  0.00345682  0.46485537] -> 1.0\n",
      "[-0.04669056 -0.23403034  0.01275393  0.17326403] -> 1.0\n",
      "[-0.05137117 -0.03909322  0.01621921 -0.11536831] -> 1.0\n",
      "[-0.05215304 -0.23444377  0.01391184  0.1823872 ] -> 1.0\n",
      "[-0.05684191 -0.429762    0.01755958  0.47942615] -> 1.0\n",
      "[-0.06543715 -0.6251274   0.02714811  0.7775913 ] -> 1.0\n",
      "[-0.0779397  -0.82061195  0.04269993  1.0786906 ] -> 1.0\n",
      "[-0.09435194 -1.016271    0.06427374  1.3844616 ] -> 1.0\n",
      "[-0.11467735 -1.2121329   0.09196298  1.6965314 ] -> 1.0\n",
      "[-0.13892001 -1.0181843   0.12589361  1.4338367 ] -> 1.0\n",
      "[-0.1592837  -0.82482     0.15457034  1.1830009 ] -> 1.0\n",
      "[-0.1757801  -0.6320039   0.17823036  0.94248784] -> 1.0\n",
      "[-0.18842018 -0.8290214   0.19708012  1.2854553 ] -> 1.0\n",
      "[-0.20500061 -0.6368766   0.22278923  1.0603846 ] -> 1.0\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "\n",
    "done = False\n",
    "while not done:\n",
    "   env.render()\n",
    "   obs, rew, done, info, _ = env.step(env.action_space.sample())\n",
    "   print(f\"{obs} -> {rew}\")\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get min and max value of those numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38]\n",
      "[4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38]\n"
     ]
    }
   ],
   "source": [
    "print(env.observation_space.low)\n",
    "print(env.observation_space.high)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State Discretization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discretize(x):\n",
    "    return tuple((x/np.array([0.25, 0.25, 0.01, 0.1])).astype(np.int))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also explore other discretization method using bins:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample bins for interval (-5,5) with 10 bins\n",
      " [-5. -4. -3. -2. -1.  0.  1.  2.  3.  4.  5.]\n"
     ]
    }
   ],
   "source": [
    "def create_bins(i,num):\n",
    "    return np.arange(num+1)*(i[1]-i[0])/num+i[0]\n",
    "\n",
    "print(\"Sample bins for interval (-5,5) with 10 bins\\n\",create_bins((-5,5),10))\n",
    "\n",
    "ints = [(-5,5),(-3.5,3.5),(-0.5,0.5),(-2,2)] # intervals of values for each parameter\n",
    "nbins = [20,20,10,10] # number of bins for each parameter\n",
    "bins = [create_bins(ints[i],nbins[i]) for i in range(4)]\n",
    "\n",
    "def discretize_bins(x):\n",
    "    return tuple(np.digitize(x[i],bins[i]) for i in range(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now run a short simulation and observe those discrete environment values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0, 3, -2)\n",
      "(0, 1, 2, -5)\n",
      "(0, 0, 1, -2)\n",
      "(0, 1, 1, -5)\n",
      "(0, 2, 0, -8)\n",
      "(0, 3, -1, -10)\n",
      "(0, 4, -3, -13)\n",
      "(0, 3, -6, -11)\n",
      "(0, 2, -8, -8)\n",
      "(0, 1, -10, -5)\n",
      "(0, 2, -11, -8)\n",
      "(0, 1, -12, -6)\n",
      "(0, 0, -14, -3)\n",
      "(0, 0, -14, -1)\n",
      "(0, 0, -15, -4)\n",
      "(0, 1, -16, -8)\n",
      "(0, 0, -17, -5)\n",
      "(0, 1, -19, -9)\n",
      "(0, 0, -20, -6)\n",
      "(0, 1, -22, -10)\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "\n",
    "done = False\n",
    "while not done:\n",
    "   #env.render()\n",
    "   obs, rew, done, info, _ = env.step(env.action_space.sample())\n",
    "   #print(discretize_bins(obs))\n",
    "   print(discretize(obs))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Table Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = {}\n",
    "actions = (0,1)\n",
    "\n",
    "def qvalues(state):\n",
    "    return [Q.get((state,a),0) for a in actions]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's Start Q-Learning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "alpha = 0.3\n",
    "gamma = 0.9\n",
    "epsilon = 0.\n",
    "\n",
    "def probs(v,eps=1e-4):\n",
    "    v = v-v.min()+eps\n",
    "    v = v/v.sum()\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "Qmax = 0\n",
    "cum_rewards = []\n",
    "rewards = []\n",
    "for epoch in range(100000):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    cum_reward=0\n",
    "    \n",
    "    # == do the simulation ==\n",
    "    while not done:\n",
    "        print(obs[0])\n",
    "        s = discretize(obs[0])\n",
    "        if random.random()<epsilon:\n",
    "            # exploitation - chose the action according to Q-Table probabilities\n",
    "            v = probs(np.array(qvalues(s)))\n",
    "            a = random.choices(actions,weights=v)[0]\n",
    "        else:\n",
    "            # exploration - randomly chose the action\n",
    "            a = np.random.randint(env.action_space.n)\n",
    "\n",
    "        obs, rew, done, info, _ = env.step(a)\n",
    "        cum_reward+=rew\n",
    "        ns = discretize(obs)\n",
    "        Q[(s,a)] = (1 - alpha) * Q.get((s,a),0) + alpha * (rew + gamma * max(qvalues(ns)))\n",
    "    \n",
    "    cum_rewards.append(cum_reward)\n",
    "    rewards.append(cum_reward)\n",
    "    \n",
    "    # == Periodically print results and calculate average reward ==\n",
    "    if epoch%5000==0:\n",
    "        print(f\"{epoch}: {np.average(cum_rewards)}, alpha={alpha}, epsilon={epsilon}\")\n",
    "        if np.average(cum_rewards) > Qmax:\n",
    "            Qmax = np.average(cum_rewards)\n",
    "            Qbest = Q\n",
    "        cum_rewards=[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting Training Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this graph, it is not possible to tell anything, because due to the nature of stochastic training process the length of training sessions varies greatly. To make more sense of this graph, we can calculate **running average** over series of experiments, let's say 100. This can be done conveniently using `np.convolve`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def running_average(x,window):\n",
    "    return np.convolve(x,np.ones(window)/window,mode='valid')\n",
    "\n",
    "plt.plot(running_average(rewards,100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Varying Hyperparameters and Seeing the Result in Action\n",
    "\n",
    "Now it would be interesting to actually see how the trained model behaves. Let's run the simulation, and we will be following the same action selection strategy as during training: sampling according to the probability distribution in Q-Table: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()\n",
    "done = False\n",
    "while not done:\n",
    "   s = discretize(obs)\n",
    "   env.render()\n",
    "   v = probs(np.array(qvalues(s)))\n",
    "   a = random.choices(actions,weights=v)[0]\n",
    "   obs,_,done,_ = env.step(a)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Saving result to an animated GIF\n",
    "\n",
    "If you want to impress your friends, you may want to send them the animated GIF picture of the balancing pole. To do this, we can invoke `env.render` to produce an image frame, and then save those to animated GIF using PIL library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Qbest' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-68c28af8d895>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m    \u001b[0mimg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m    \u001b[0mims\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m    \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprobs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mQbest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m    \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m    \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-68c28af8d895>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      9\u001b[0m    \u001b[0mimg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m    \u001b[0mims\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m    \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprobs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mQbest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m    \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m    \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Qbest' is not defined"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "env = gym.make(\"CartPole-v1\", render_mode='rgb_array')\n",
    "obs = env.reset()\n",
    "done = False\n",
    "i=0\n",
    "ims = []\n",
    "while not done:\n",
    "   s = discretize(obs[0])\n",
    "   img=env.render()\n",
    "   ims.append(Image.fromarray(img))\n",
    "   v = probs(np.array([Qbest.get((s,a),0) for a in actions]))\n",
    "   a = random.choices(actions,weights=v)[0]\n",
    "   obs,_,done,_ = env.step(a)\n",
    "   i+=1\n",
    "env.close()\n",
    "ims[0].save('images/cartpole-balance.gif',save_all=True,append_images=ims[1::2],loop=0,duration=5)\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "70b38d7a306a849643e446cd70466270a13445e5987dfa1344ef2b127438fa4d"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
